# -*- coding: utf-8 -*-
"""Decision Tree and Random Forest Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15BHA3VXNgAsKqaXsr0mtW1MdDbrZJ0Fo
"""

# ==============================================================================
# Task 5: Decision Trees and Random Forests
# This script performs all required steps for the assignment:
# 1. Reliable data loading and cleaning.
# 2. Training constrained Decision Tree (to prevent overfitting).
# 3. Training Random Forest.
# 4. 10-Fold Cross-Validation for robust performance evaluation.
# 5. Visualization of the Decision Tree.
# 6. Feature Importance plot from Random Forest.
# ==============================================================================

# Install required library for visualizing the decision tree (Graphviz)
# This handles the most common installation dependency in Colab.
!pip install graphviz

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import graphviz
from IPython.display import display

# ==============================================================================
# SECTION 1: Data Loading, Cleaning, and Preparation (Robust Version)
# ==============================================================================

# URL for the raw Cleveland Heart Disease Data (it does not have column headers)
DATA_URL = "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"

# Manually define column names as the raw file lacks them
column_names = [
    'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',
    'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'
]

print("Attempting to load data from URL...")
try:
    # Load the data without a header, and assign column names
    df = pd.read_csv(DATA_URL, names=column_names, na_values='?')
    print("Dataset loaded successfully.")

    # Convert features with missing values ('ca' and 'thal') to numeric, forcing errors to NaN
    df['ca'] = pd.to_numeric(df['ca'], errors='coerce')
    df['thal'] = pd.to_numeric(df['thal'], errors='coerce')

    # Drop rows with any missing values (Handling '?' that were converted to NaN)
    df.dropna(inplace=True)
    print(f"Data cleaned. Remaining records: {len(df)}")

    # The original dataset defines 'target' as 0, 1, 2, 3, 4. We simplify to a binary classification:
    # 0 = No disease, 1 = Disease (i.e., target > 0)
    df['target'] = df['target'].apply(lambda x: 1 if x > 0 else 0)

except Exception as e:
    print(f"\nFATAL ERROR during data loading or cleaning: {e}")
    print("Please ensure your internet connection is stable.")
    # Stop execution if data isn't loaded/cleaned
    raise

# Define Features (X) and Target (y)
X = df.drop('target', axis=1)
y = df['target']
feature_names = X.columns.tolist()
class_names = ['No Disease', 'Disease']

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Train set size: {len(X_train)}, Test set size: {len(X_test)}")


# ==============================================================================
# SECTION 2: Decision Tree Classifier
# ==============================================================================

print("\n" + "="*50)
print("STARTING DECISION TREE CLASSIFIER ANALYSIS")
print("="*50)

# 2a. Train a MAX-DEPTH Decision Tree (for observing overfitting)
dt_full = DecisionTreeClassifier(random_state=42)
dt_full.fit(X_train, y_train)

# Check for overfitting
print(f"Full DT Training Accuracy: {dt_full.score(X_train, y_train):.4f}")
print(f"Full DT Test Accuracy: {dt_full.score(X_test, y_test):.4f}")
print("-> Note: Training accuracy near 1.000 suggests potential overfitting (Hint 2).")


# 2b. Train a CONSTRAINED Decision Tree (to prevent overfitting - Hint 2)
# We limit the depth to 4 for simplicity and better visualization.
dt_constrained = DecisionTreeClassifier(max_depth=4, random_state=42)
dt_constrained.fit(X_train, y_train)
y_pred_dt = dt_constrained.predict(X_test)
dt_accuracy = accuracy_score(y_test, y_pred_dt)

print(f"\nConstrained DT (Max Depth 4) Test Accuracy: {dt_accuracy:.4f}")
print("\nClassification Report (Decision Tree):")
print(classification_report(y_test, y_pred_dt))


# 2c. Visualize the Constrained Decision Tree (Hint 1 & 6)
print("\nGenerating Decision Tree Visualization (Max Depth 4)...")
dot_data = export_graphviz(
    dt_constrained,
    out_file=None,
    feature_names=feature_names,
    class_names=class_names,
    filled=True,
    rounded=True,
    special_characters=True
)
graph = graphviz.Source(dot_data)
# Display the graph in the notebook output
display(graph)
print("Visualization generated above.")


# ==============================================================================
# SECTION 3: Random Forest Classifier and Feature Importance
# ==============================================================================

print("\n" + "="*50)
print("STARTING RANDOM FOREST CLASSIFIER ANALYSIS")
print("="*50)

# 3a. Train a Random Forest Classifier (Hint 3)
rf_model = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, y_pred_rf)

print(f"Random Forest Test Accuracy: {rf_accuracy:.4f}")
print("\nClassification Report (Random Forest):")
print(classification_report(y_test, y_pred_rf))


# 3b. Interpret Feature Importances (Hint 4 & 7)
importances = rf_model.feature_importances_
feature_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)

print("\nTop 5 Feature Importances:")
print(feature_importances.head())

# Plotting Feature Importance
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importances.values, y=feature_importances.index, palette="viridis")
plt.title('Feature Importance from Random Forest Model', fontsize=16)
plt.xlabel('Importance Score', fontsize=12)
plt.ylabel('Feature', fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()
print("Feature Importance plot generated above.")


# ==============================================================================
# SECTION 4: Cross-Validation (Final Robust Evaluation)
# ==============================================================================

print("\n" + "="*50)
print("STARTING 10-FOLD CROSS-VALIDATION (Hint 5)")
print("="*50)

# Use Random Forest for CV as it is usually the stronger model
cv_scores = cross_val_score(rf_model, X, y, cv=10, scoring='accuracy')

print(f"Individual 10-Fold CV Scores:\n{cv_scores}")
print(f"\nMean CV Accuracy (Random Forest): {cv_scores.mean():.4f}")
print(f"Standard Deviation of CV Scores: {cv_scores.std():.4f}")
print("\nFinal CV Score is the most robust measure for your report.")

# ==============================================================================
# END OF SCRIPT
# ==============================================================================

from google.colab import drive
drive.mount('/content/drive')